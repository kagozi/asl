# ============================================================================
# config/baseline_config.yaml
# ============================================================================
# Configuration for baseline transformer model

model:
  name: "baseline_transformer"
  embedding_dim: 512
  num_heads: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  ffn_dim: 2048  # 4x embedding_dim for ReLU
  dropout: 0.1
  max_seq_len: 512
  activation: "relu"
  norm_type: "layer_norm"
  position_encoding: "sinusoidal"

data:
  dataset_name: "achrafothman/aslg_pc12"
  cache_dir: "./data/cache"
  train_size: 82710
  val_size: 4000
  test_size: 4145
  max_length: 100
  
  # Preprocessing
  lowercase_text: true
  uppercase_gloss: true
  remove_punctuation: true
  remove_digits: true

training:
  # Basic settings
  batch_size: 32
  gradient_accumulation_steps: 64  # Effective batch size = 2048
  epochs: 100
  max_steps: null  # If set, overrides epochs
  
  # Optimization
  optimizer: "adam"
  learning_rate: 0.0  # Will be calculated by scheduler
  betas: [0.9, 0.98]
  eps: 1.0e-9
  weight_decay: 0.01
  
  # Learning rate schedule
  scheduler: "noam"
  warmup_steps: 4000
  lr_factor: 1.0
  
  # Regularization
  label_smoothing: 0.1
  gradient_clip_norm: 1.0
  gradient_clip_value: null
  
  # Mixed precision
  use_amp: true
  amp_dtype: "bfloat16"  # or "float16"
  
  # Checkpointing
  save_every_n_epochs: 5
  keep_last_n_checkpoints: 3
  save_best_only: true
  early_stopping_patience: 10

evaluation:
  metrics:
    - "bleu"
    - "meteor"
    - "chrf"
    - "rouge"
  beam_size: 5
  length_penalty: 0.6
  max_generation_length: 100
  num_eval_examples: null  # null = evaluate on full test set

logging:
  log_every_n_steps: 10
  eval_every_n_epochs: 1
  use_tensorboard: true
  use_wandb: false
  wandb_project: "text-gloss-translation"
  wandb_entity: null

compute:
  device: "cuda"  # or "cpu"
  num_workers: 4
  pin_memory: true
  compile_model: false  # PyTorch 2.0+ compilation

seed: 42